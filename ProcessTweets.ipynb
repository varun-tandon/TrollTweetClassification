{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forensic Linguistics Applied To Tweets By Bots and Trolls\n",
    "Project by Varun Tandon as a part of Stanford's CS109 Final Project. \n",
    "\n",
    "**Warning: Since much of this uncensored data was obtained from troll/bot accounts as well as the general Twittersphere, there may be profane, racist, sexist, or other inflammatory content shown as output.** The output of snippets of code and the content of the data processed are not indicative of my personal views. All forms of bigotry should be condemned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helpful Functions\n",
    "\n",
    "I began by writing up some helpful functions for processing and cleaning the data that I had gathered. Since I'm running an analysis on tweets, which can contain anything from Chinese characters to emoji, I needed a function to clean up a tweet by converting it to alphabetic letters. \n",
    "\n",
    "I also wrote a function for generating word counts and frequencies, with the hope that I could use the differences in word frequencies between the two sets to classify unknown tweets (similar to the Federalist Papers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(col):\n",
    "    col = col.apply(lambda x: str(x).lower())\n",
    "    alpha_only = re.compile('[^a-zA-Z\\s]')\n",
    "    col = col.apply(lambda x: alpha_only.sub('', x))\n",
    "    \n",
    "    col = col.apply(lambda x: str(x).split())\n",
    "    return col\n",
    "\n",
    "def generate_word_count(col):\n",
    "    word_count = dict()\n",
    "    for i in range(col.size):\n",
    "        for word in col[i]:\n",
    "            if (word in word_count.keys()):\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    return word_count\n",
    "\n",
    "def generate_word_freq(count, total):\n",
    "    return (count / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data (Troll)\n",
    "\n",
    "The data used here was acquired from a Kaggle dataset of tweets that are known to be posted by Russian bots/trolls. The dataset can be found here: https://www.kaggle.com/vikasg/russian-troll-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "troll_df = pd.read_csv('tweets.csv')\n",
    "\n",
    "# Clean the text\n",
    "troll_df['text'] = clean_tweet(troll_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice \"rt\" ranking as the most frequent word here, and in this case \"rt\" indicates a \"retweet\" by a Twitter user. At this point, I'm not sure whether or not to leave it in. On the one hand, perhaps a bot is more/less likely to retweet. On the other hand, this might just ruin the accuracy of predictions because it doesn't have any bearing on the content of the tweet. \n",
    "\n",
    "For now, I'm going to leave it in, and then later on I'm going to see how it affects the prediction accuracy to remove it. \n",
    "\n",
    "We also notice that \"trump\" and \"clinton\" rank fairly highly on the word frequencies list. This seems to be a good sign, because assuming that tweets are similar to normal human language, these should not be very frequent terms in normal tweets. We'll verify that hypothesis when generating word frequencies on a random sample of Twitter data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data (Rest of Twitter)\n",
    "\n",
    "Ideally I would have found some Twitter data from the same timeframe as the Russian data (as tweets will often skew according to current events); however, I could not find any unbiased, random datasources containing data from the same time frame. Most of the datasets on Kaggle tend to have some focus (ie. tweets from Russian trolls, unhappy tweets, etc.), so I had to generate my own random sample. \n",
    "\n",
    "To do so, I used the twarc command line tool. \n",
    "\n",
    "Specifically, I sampled on Wednesday, May 29th, 2018, with the following command:\n",
    "\n",
    "twarc sample > tweets.jsonl\n",
    "\n",
    "Unfortunately, there's no way for me to select just English tweets using this, so of the 70,747 tweets extracted, only 22,595 are in English. Still, this is a sizeable number of tweets, and hopefully this provides a good representation of tweet word frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the JSON of random tweets\n",
    "twitter_df = pd.read_json('tweets.jsonl', lines=True)\n",
    "\n",
    "# Isolate English tweets\n",
    "twitter_df = twitter_df[twitter_df.lang == 'en'].reset_index()\n",
    "\n",
    "# Clean the tweets\n",
    "twitter_df['text'] = clean_tweet(twitter_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Evaluation Sets\n",
    "\n",
    "In order to evaluate the accuracy of my system of classifying tweets as troll or normal, I need to have some unbiased tweets to classify whose answers I know. To generate this set, I will randomly remove 2500 tweets from both the normal and troll datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random indices to isolate\n",
    "norm_to_remove = np.random.choice(twitter_df.shape[0], 2500)\n",
    "troll_to_remove = np.random.choice(troll_df.shape[0], 2500)\n",
    "\n",
    "norm_test = pd.DataFrame(twitter_df['text'].iloc[norm_to_remove])\n",
    "norm_test['is_troll'] = False\n",
    "troll_test = pd.DataFrame(troll_df['text'].iloc[troll_to_remove])\n",
    "troll_test['is_troll'] = True\n",
    "\n",
    "test_data = norm_test.append(troll_test).reset_index()\n",
    "\n",
    "twitter_df = twitter_df.drop(norm_to_remove).reset_index()\n",
    "troll_df = troll_df.drop(troll_to_remove).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Word Frequencies For Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>word_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>147967</td>\n",
       "      <td>5.272635e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>69279</td>\n",
       "      <td>2.468678e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>55427</td>\n",
       "      <td>1.975078e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>38941</td>\n",
       "      <td>1.387618e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>33856</td>\n",
       "      <td>1.206420e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>32325</td>\n",
       "      <td>1.151864e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>29379</td>\n",
       "      <td>1.046887e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>27359</td>\n",
       "      <td>9.749066e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>26415</td>\n",
       "      <td>9.412683e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>24815</td>\n",
       "      <td>8.842541e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>22607</td>\n",
       "      <td>8.055746e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>20279</td>\n",
       "      <td>7.226189e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>18281</td>\n",
       "      <td>6.514225e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>13796</td>\n",
       "      <td>4.916047e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>13247</td>\n",
       "      <td>4.720417e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>13148</td>\n",
       "      <td>4.685139e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>11964</td>\n",
       "      <td>4.263234e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>11538</td>\n",
       "      <td>4.111434e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>11286</td>\n",
       "      <td>4.021637e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinton</th>\n",
       "      <td>10870</td>\n",
       "      <td>3.873400e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hillary</th>\n",
       "      <td>10470</td>\n",
       "      <td>3.730865e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>10142</td>\n",
       "      <td>3.613986e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>9108</td>\n",
       "      <td>3.245532e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>9073</td>\n",
       "      <td>3.233060e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>9052</td>\n",
       "      <td>3.225577e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>8906</td>\n",
       "      <td>3.173551e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>8635</td>\n",
       "      <td>3.076983e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>8330</td>\n",
       "      <td>2.968300e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>8319</td>\n",
       "      <td>2.964380e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obama</th>\n",
       "      <td>8031</td>\n",
       "      <td>2.861755e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrsrep</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderfulwomank</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcooszfzksnv</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcozwpaigfw</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcohsgpmtrms</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facthillary</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcovznhvcsliu</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dwpolitics</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcowyelrmo</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcoculpqbpl</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>umtata</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verflixt</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstconurrgwyyy</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bemht</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autoren</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lesenswert</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tribalnationtni</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcojuipdbpup</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apadeo</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcosxwgtqxne</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heisyourpres</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dramatics</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcoorjxjb</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcogpvghbfh</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcoxfuggvpoj</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>francisski</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sosokoba</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>releasethetranscripts</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcozoqzkqtyt</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qual</th>\n",
       "      <td>1</td>\n",
       "      <td>3.563386e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234118 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0     word_freq\n",
       "rt                     147967  5.272635e-02\n",
       "the                     69279  2.468678e-02\n",
       "to                      55427  1.975078e-02\n",
       "a                       38941  1.387618e-02\n",
       "of                      33856  1.206420e-02\n",
       "in                      32325  1.151864e-02\n",
       "is                      29379  1.046887e-02\n",
       "trump                   27359  9.749066e-03\n",
       "and                     26415  9.412683e-03\n",
       "for                     24815  8.842541e-03\n",
       "you                     22607  8.055746e-03\n",
       "i                       20279  7.226189e-03\n",
       "on                      18281  6.514225e-03\n",
       "this                    13796  4.916047e-03\n",
       "that                    13247  4.720417e-03\n",
       "it                      13148  4.685139e-03\n",
       "with                    11964  4.263234e-03\n",
       "are                     11538  4.111434e-03\n",
       "be                      11286  4.021637e-03\n",
       "clinton                 10870  3.873400e-03\n",
       "hillary                 10470  3.730865e-03\n",
       "not                     10142  3.613986e-03\n",
       "we                       9108  3.245532e-03\n",
       "amp                      9073  3.233060e-03\n",
       "at                       9052  3.225577e-03\n",
       "my                       8906  3.173551e-03\n",
       "your                     8635  3.076983e-03\n",
       "have                     8330  2.968300e-03\n",
       "will                     8319  2.964380e-03\n",
       "obama                    8031  2.861755e-03\n",
       "...                       ...           ...\n",
       "mrsrep                      1  3.563386e-07\n",
       "wonderfulwomank             1  3.563386e-07\n",
       "httpstcooszfzksnv           1  3.563386e-07\n",
       "httpstcozwpaigfw            1  3.563386e-07\n",
       "httpstcohsgpmtrms           1  3.563386e-07\n",
       "facthillary                 1  3.563386e-07\n",
       "httpstcovznhvcsliu          1  3.563386e-07\n",
       "dwpolitics                  1  3.563386e-07\n",
       "httpstcowyelrmo             1  3.563386e-07\n",
       "httpstcoculpqbpl            1  3.563386e-07\n",
       "umtata                      1  3.563386e-07\n",
       "verflixt                    1  3.563386e-07\n",
       "httpstconurrgwyyy           1  3.563386e-07\n",
       "bemht                       1  3.563386e-07\n",
       "autoren                     1  3.563386e-07\n",
       "lesenswert                  1  3.563386e-07\n",
       "tribalnationtni             1  3.563386e-07\n",
       "httpstcojuipdbpup           1  3.563386e-07\n",
       "apadeo                      1  3.563386e-07\n",
       "httpstcosxwgtqxne           1  3.563386e-07\n",
       "heisyourpres                1  3.563386e-07\n",
       "dramatics                   1  3.563386e-07\n",
       "httpstcoorjxjb              1  3.563386e-07\n",
       "httpstcogpvghbfh            1  3.563386e-07\n",
       "httpstcoxfuggvpoj           1  3.563386e-07\n",
       "francisski                  1  3.563386e-07\n",
       "sosokoba                    1  3.563386e-07\n",
       "releasethetranscripts       1  3.563386e-07\n",
       "httpstcozoqzkqtyt           1  3.563386e-07\n",
       "qual                        1  3.563386e-07\n",
       "\n",
       "[234118 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate word counts\n",
    "word_count = generate_word_count(troll_df['text'])\n",
    "\n",
    "# Generate the word frequencies\n",
    "troll_wf = pd.DataFrame.from_dict(word_count, orient='index').sort_values(by=[0], ascending=False)\n",
    "total_count = sum(troll_wf[0])\n",
    "troll_wf['word_freq'] = troll_wf.apply(lambda x: generate_word_freq(x, total_count))\n",
    "\n",
    "troll_wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>word_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>12503</td>\n",
       "      <td>0.043709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>7648</td>\n",
       "      <td>0.026736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>5726</td>\n",
       "      <td>0.020017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>5124</td>\n",
       "      <td>0.017913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4972</td>\n",
       "      <td>0.017381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>3866</td>\n",
       "      <td>0.013515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3772</td>\n",
       "      <td>0.013186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>3280</td>\n",
       "      <td>0.011466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>3175</td>\n",
       "      <td>0.011099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>2898</td>\n",
       "      <td>0.010131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>2724</td>\n",
       "      <td>0.009523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>2509</td>\n",
       "      <td>0.008771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>2364</td>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>2141</td>\n",
       "      <td>0.007485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>2031</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1950</td>\n",
       "      <td>0.006817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>1911</td>\n",
       "      <td>0.006681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>1438</td>\n",
       "      <td>0.005027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>1402</td>\n",
       "      <td>0.004901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>im</th>\n",
       "      <td>1340</td>\n",
       "      <td>0.004684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>1305</td>\n",
       "      <td>0.004562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>1199</td>\n",
       "      <td>0.004192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>1187</td>\n",
       "      <td>0.004150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if</th>\n",
       "      <td>1156</td>\n",
       "      <td>0.004041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>1153</td>\n",
       "      <td>0.004031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>1151</td>\n",
       "      <td>0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>1097</td>\n",
       "      <td>0.003835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>1081</td>\n",
       "      <td>0.003779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>1054</td>\n",
       "      <td>0.003685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>1040</td>\n",
       "      <td>0.003636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kittytranny</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stephan</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euvaille</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assignments</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penmanship</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killajunsters</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ageedior</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcodmzjgev</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinnybrack</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michaelme</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calicurmudgeon</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>billyever</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>einsteinmaga</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americanalina</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcopzxxwknf</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstconykbifxhf</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naughtyk</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adrianfknpetrov</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stic</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deaths</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tamnna</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aquariusunite</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irukkura</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcojsgscndh</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcosaparugyf</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chua</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>welson</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>barnabychuck</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elmuss</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpstcomnmlxphv</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43453 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0  word_freq\n",
       "rt                 12503   0.043709\n",
       "the                 7648   0.026736\n",
       "to                  5726   0.020017\n",
       "i                   5124   0.017913\n",
       "a                   4972   0.017381\n",
       "you                 3866   0.013515\n",
       "and                 3772   0.013186\n",
       "is                  3280   0.011466\n",
       "of                  3175   0.011099\n",
       "in                  2898   0.010131\n",
       "this                2724   0.009523\n",
       "for                 2509   0.008771\n",
       "my                  2364   0.008264\n",
       "me                  2141   0.007485\n",
       "that                2031   0.007100\n",
       "it                  1950   0.006817\n",
       "on                  1911   0.006681\n",
       "be                  1438   0.005027\n",
       "with                1402   0.004901\n",
       "im                  1340   0.004684\n",
       "so                  1305   0.004562\n",
       "your                1199   0.004192\n",
       "not                 1187   0.004150\n",
       "if                  1156   0.004041\n",
       "are                 1153   0.004031\n",
       "like                1151   0.004024\n",
       "have                1097   0.003835\n",
       "at                  1081   0.003779\n",
       "just                1054   0.003685\n",
       "but                 1040   0.003636\n",
       "...                  ...        ...\n",
       "kittytranny            1   0.000003\n",
       "stephan                1   0.000003\n",
       "euvaille               1   0.000003\n",
       "assignments            1   0.000003\n",
       "penmanship             1   0.000003\n",
       "killajunsters          1   0.000003\n",
       "ageedior               1   0.000003\n",
       "httpstcodmzjgev        1   0.000003\n",
       "vinnybrack             1   0.000003\n",
       "michaelme              1   0.000003\n",
       "calicurmudgeon         1   0.000003\n",
       "billyever              1   0.000003\n",
       "einsteinmaga           1   0.000003\n",
       "americanalina          1   0.000003\n",
       "httpstcopzxxwknf       1   0.000003\n",
       "httpstconykbifxhf      1   0.000003\n",
       "naughtyk               1   0.000003\n",
       "adrianfknpetrov        1   0.000003\n",
       "stic                   1   0.000003\n",
       "deaths                 1   0.000003\n",
       "tamnna                 1   0.000003\n",
       "aquariusunite          1   0.000003\n",
       "irukkura               1   0.000003\n",
       "httpstcojsgscndh       1   0.000003\n",
       "httpstcosaparugyf      1   0.000003\n",
       "chua                   1   0.000003\n",
       "welson                 1   0.000003\n",
       "barnabychuck           1   0.000003\n",
       "elmuss                 1   0.000003\n",
       "httpstcomnmlxphv       1   0.000003\n",
       "\n",
       "[43453 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate word counts\n",
    "twitter_word_count = generate_word_count(twitter_df['text'])\n",
    "\n",
    "# Generate word frequencies\n",
    "twit_wf = pd.DataFrame.from_dict(twitter_word_count, orient='index').sort_values(by=[0], ascending=False)\n",
    "total_count = sum(twit_wf[0])\n",
    "twit_wf['word_freq'] = twit_wf.apply(lambda x: generate_word_freq(x, total_count))\n",
    "\n",
    "twit_wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Consider the words in a given tweet as $T$. Let us also denote the troll/bot writer as $B$ and the normal twitter user as $N$. \n",
    "\n",
    "We want to find\n",
    "\n",
    "$$\\frac{P(N|T)}{P(B|T)}$$  \n",
    "$$ = \\frac{P(T|N)P(N)}{P(T|B)P(B)}$$\n",
    "\n",
    "Googling around for the percentage of tweets posted by bots indicates some alarming statistics (see: https://www.pewresearch.org/fact-tank/2018/04/09/5-things-to-know-about-bots-on-twitter/), but none of these statistics give a valid prior for the probability of a tweet being posted by a bot. To represent this ambiguity, we will say that\n",
    "\n",
    "$$P(N) = P(B) = 0.5$$\n",
    "\n",
    "We observe that this cancels out in our equation above, so we are left with\n",
    "\n",
    "$$\\frac{P(T|N)}{P(T|B)}$$\n",
    "\n",
    "As we did in class, we can rewrite these using multinomials, and the multinomial terms in the numerator and denominator cancel, yielding\n",
    "\n",
    "$$\\frac{\\prod_i p_{i}^{c_i}}{\\prod_i q_{i}^{c_i}}$$\n",
    "\n",
    "We can use logarithms to make this computationally stable, and write\n",
    "\n",
    "$$\\log(\\frac{P(T|N)}{P(T|B)}) = \\log(\\frac{\\prod_i p_{i}^{c_i}}{\\prod_i q_{i}^{c_i}}) = \\sum_i c_i\\log(p_i) - \\sum_i c_i \\log(q_i)$$\n",
    "\n",
    "(To reiterate, this process is identical to the process done in lecture with the Federalist Papers, so some steps in the math were ommited)\n",
    "\n",
    "Now to convert this to code!\n",
    "\n",
    "The one issue that we run into is the case where a Twitter user writes a unique word, a word that has not been used by a bot or a normal human in our datasets. In this case, I simply assume a word frequency equal to 0.000004, which is the frequency for words that are observed once in the normal tweets word frequency table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Tweets: tested 2500, correctly classified 2282.\n",
      "Troll Tweets: tested 2500, correctly classified 1818.\n",
      "Overall: tested 5000, correctly classified 4100.\n",
      "Accuracy = 0.82.\n"
     ]
    }
   ],
   "source": [
    "def calculate_LL(word_list, is_bot):\n",
    "    unique_word_error = 0.000004\n",
    "    sum = 0\n",
    "    freq_list = None\n",
    "    if is_bot:\n",
    "        freq_list = troll_wf['word_freq']\n",
    "    else:\n",
    "        freq_list = twit_wf['word_freq']\n",
    "    for word in word_list:\n",
    "        if (word in freq_list):\n",
    "            sum += np.log(freq_list[word])\n",
    "        else:\n",
    "            sum += np.log(unique_word_error)\n",
    "    return sum\n",
    "\n",
    "def is_troll(likelihood_norm):\n",
    "    if (likelihood_norm > 1):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def print_results(data):\n",
    "    class0_count = data[data['is_troll'] == False].shape[0]\n",
    "    class1_count = data[data['is_troll'] == True].shape[0]\n",
    "\n",
    "    class0_correct = data[(data['pred_correct'] == True) & (data['is_troll'] == 0)].shape[0]\n",
    "    class1_correct = data[(data['pred_correct'] == True) & (data['is_troll'] == 1)].shape[0]\n",
    "\n",
    "    print(\"Normal Tweets: tested {}, correctly classified {}.\".format(class0_count, class0_correct))\n",
    "    print(\"Troll Tweets: tested {}, correctly classified {}.\".format(class1_count, class1_correct))\n",
    "    print(\"Overall: tested {}, correctly classified {}.\".format(class0_count + class1_count, class0_correct + class1_correct))\n",
    "    print(\"Accuracy = {}.\".format((class0_correct + class1_correct) / (class0_count + class1_count)))\n",
    "\n",
    "test_data['norm_LL'] = test_data['text'].apply(lambda x: calculate_LL(x, False))\n",
    "test_data['bot_LL'] = test_data['text'].apply(lambda x: calculate_LL(x, True))\n",
    "test_data['norm_LL - bot_LL'] = test_data['norm_LL'] - test_data['bot_LL']\n",
    "test_data['e^(prob)'] = np.exp(test_data['norm_LL - bot_LL'])\n",
    "test_data['pred_is_troll'] = test_data['e^(prob)'].apply(is_troll)\n",
    "test_data['pred_correct'] = test_data.apply(lambda x: x['is_troll'] == x['pred_is_troll'], axis=1)\n",
    "\n",
    "print_results(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking To The Future\n",
    "\n",
    "While an accuracy of 0.82 is pretty good, I'm sure there are a lot of adjustments that can be made to improve the accuracy of the predictions. \n",
    "\n",
    "### 1. Word Similarity\n",
    "Right now the comparisons are being run using distinct words, rather than clustering similar words. For example, we notice that at the tail end of the troll tweets there are a lot of URLs. A study by Pew (linked above) found that 66% of all links posted on Twitter were from trolls, so clustering URLs could be provide a boost to accuracy. \n",
    "\n",
    "Along the same lines, hashtags tend to join words together, so clustering hashtags with their composite words could improve the model's understanding of what is being talked about. \n",
    "\n",
    "### 2. Retweets\n",
    "As I alluded to earlier, it could be interesting to try removing and adding retweets to see if this is a practice that is more common among bots than humans. \n",
    "\n",
    "### 3. Other Algorithms\n",
    "Since I started this project, CS109 has explored logistic regression and neural networks, whereas the approach taken in this project was more along the lines of Naive Bayes. Perhaps trying logistic regression and neural nets would have higher accuracy in predicting whether a user is a bot. \n",
    "\n",
    "### 4. More Data\n",
    "I was somewhat limited by the processing power of my laptop and the API rate limits imposed by Twitter. Since the publication of the Kaggle dataset I used in this project, there has been the publication of a new dataset with over 3 million troll tweets (https://www.kaggle.com/fivethirtyeight/russian-troll-tweets/version/1). \n",
    "\n",
    "Similarly, the \"control\" dataset can definitely be expanded, and increasing this dataset size would likely give a more accurate picture of the norm of word frequencies on Twitter. \n",
    "\n",
    "### 5. A Big Picture Look\n",
    "This project mainly focused on word frequencies as a mechanism for determining whether a Twitter user was a troll or not, and 8.7% of the tweets in the test set were labelled incorrectly as troll. Analyzing these false positive could give a good picture of what this model is really predicting. Are normal users who tweet about politics being labelled as bots? Is the model simply classifying content as toxic/bigoted? Looking at what's going on with some individual cases could indicate more accurately what content is being labelled as troll. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
