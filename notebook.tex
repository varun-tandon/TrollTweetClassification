
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{ProcessTweets}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{federalists-papers-meets-russian-trolls-forensic-linguistics-applied-to-tweets}{%
\section{Federalists Papers meets Russian Trolls: Forensic Linguistics
Applied To
Tweets}\label{federalists-papers-meets-russian-trolls-forensic-linguistics-applied-to-tweets}}

\hypertarget{classifying-troll-tweets-using-word-frequencies}{%
\paragraph{Classifying Troll Tweets Using Word
Frequencies}\label{classifying-troll-tweets-using-word-frequencies}}

Project by Varun Tandon as a part of Stanford's CS109 Final Project.

\textbf{Warning: Since much of this uncensored data was obtained from
troll/bot accounts as well as the general Twittersphere, there may be
profane, racist, sexist, or other inflammatory content shown as output.}
The output of snippets of code and the content of the data processed are
not indicative of my personal views. All forms of bigotry should be
condemned.

\hypertarget{importssetup}{%
\paragraph{Imports/Setup}\label{importssetup}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{re}
\end{Verbatim}


    \hypertarget{some-helpful-functions}{%
\subsection{Some Helpful Functions}\label{some-helpful-functions}}

I began by writing up some helpful functions for processing and cleaning
the data that I had gathered. Since I'm running an analysis on tweets,
which can contain anything from Chinese characters to emoji, I needed a
function to clean up a tweet by converting it to alphabetic letters.

I also wrote a function for generating word counts and frequencies, with
the hope that I could use the differences in word frequencies between
the two sets to classify unknown tweets (similar to the Federalist
Papers).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k}{def} \PY{n+nf}{clean\PYZus{}tweet}\PY{p}{(}\PY{n}{col}\PY{p}{)}\PY{p}{:}
            \PY{n}{col} \PY{o}{=} \PY{n}{col}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n+nb}{str}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{alpha\PYZus{}only} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[\PYZca{}a\PYZhy{}zA\PYZhy{}Z}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{s]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{col} \PY{o}{=} \PY{n}{col}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{alpha\PYZus{}only}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{)}
            
            \PY{n}{col} \PY{o}{=} \PY{n}{col}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n+nb}{str}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{k}{return} \PY{n}{col}
        
        \PY{k}{def} \PY{n+nf}{generate\PYZus{}word\PYZus{}count}\PY{p}{(}\PY{n}{col}\PY{p}{)}\PY{p}{:}
            \PY{n}{word\PYZus{}count} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{p}{)}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{col}\PY{o}{.}\PY{n}{size}\PY{p}{)}\PY{p}{:}
                \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{col}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}
                    \PY{k}{if} \PY{p}{(}\PY{n}{word} \PY{o+ow}{in} \PY{n}{word\PYZus{}count}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                        \PY{n}{word\PYZus{}count}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                    \PY{k}{else}\PY{p}{:}
                        \PY{n}{word\PYZus{}count}\PY{p}{[}\PY{n}{word}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
            \PY{k}{return} \PY{n}{word\PYZus{}count}
        
        \PY{k}{def} \PY{n+nf}{generate\PYZus{}word\PYZus{}freq}\PY{p}{(}\PY{n}{count}\PY{p}{,} \PY{n}{total}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{p}{(}\PY{n}{count} \PY{o}{/} \PY{n}{total}\PY{p}{)}
\end{Verbatim}


    \hypertarget{clean-data-troll}{%
\subsection{Clean Data (Troll)}\label{clean-data-troll}}

The data used here was acquired from a Kaggle dataset of tweets that are
known to be posted by Russian bots/trolls. The dataset can be found
here: https://www.kaggle.com/vikasg/russian-troll-tweets

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} Read CSV}
        \PY{n}{troll\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tweets.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Clean the text}
        \PY{n}{troll\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{clean\PYZus{}tweet}\PY{p}{(}\PY{n}{troll\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    We notice ``rt'' ranking as the most frequent word here, and in this
case ``rt'' indicates a ``retweet'' by a Twitter user. At this point,
I'm not sure whether or not to leave it in. On the one hand, perhaps a
bot is more/less likely to retweet. On the other hand, this might just
ruin the accuracy of predictions because it doesn't have any bearing on
the content of the tweet.

For now, I'm going to leave it in, and then later on I'm going to see
how it affects the prediction accuracy to remove it.

We also notice that ``trump'' and ``clinton'' rank fairly highly on the
word frequencies list. This seems to be a good sign, because assuming
that tweets are similar to normal human language, these should not be
very frequent terms in normal tweets. We'll verify that hypothesis when
generating word frequencies on a random sample of Twitter data.

    \hypertarget{clean-data-rest-of-twitter}{%
\subsection{Clean Data (Rest of
Twitter)}\label{clean-data-rest-of-twitter}}

Ideally I would have found some Twitter data from the same timeframe as
the Russian data (as tweets will often skew according to current
events); however, I could not find any unbiased, random datasources
containing data from the same time frame. Most of the datasets on Kaggle
tend to have some focus (ie. tweets from Russian trolls, unhappy tweets,
etc.), so I had to generate my own random sample.

To do so, I used the twarc command line tool.

Specifically, I sampled on Wednesday, May 29th, 2018, with the following
command:

twarc sample \textgreater{} tweets.jsonl

Unfortunately, there's no way for me to select just English tweets using
this, so of the 70,747 tweets extracted, only 22,595 are in English.
Still, this is a sizeable number of tweets, and hopefully this provides
a good representation of tweet word frequencies.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} Read in the JSON of random tweets}
        \PY{n}{twitter\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}json}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tweets.jsonl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lines}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Isolate English tweets}
        \PY{n}{twitter\PYZus{}df} \PY{o}{=} \PY{n}{twitter\PYZus{}df}\PY{p}{[}\PY{n}{twitter\PYZus{}df}\PY{o}{.}\PY{n}{lang} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{en}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Clean the tweets}
        \PY{n}{twitter\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{clean\PYZus{}tweet}\PY{p}{(}\PY{n}{twitter\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \hypertarget{establishing-evaluation-sets}{%
\subsection{Establishing Evaluation
Sets}\label{establishing-evaluation-sets}}

In order to evaluate the accuracy of my system of classifying tweets as
troll or normal, I need to have some unbiased tweets to classify whose
answers I know. To generate this set, I will randomly remove 2500 tweets
from both the normal and troll datasets

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} Generate random indices to isolate}
        \PY{n}{norm\PYZus{}to\PYZus{}remove} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{twitter\PYZus{}df}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{2500}\PY{p}{)}
        \PY{n}{troll\PYZus{}to\PYZus{}remove} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{troll\PYZus{}df}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{2500}\PY{p}{)}
        
        \PY{n}{norm\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{twitter\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{norm\PYZus{}to\PYZus{}remove}\PY{p}{]}\PY{p}{)}
        \PY{n}{norm\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{k+kc}{False}
        \PY{n}{troll\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{troll\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n}{troll\PYZus{}to\PYZus{}remove}\PY{p}{]}\PY{p}{)}
        \PY{n}{troll\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{k+kc}{True}
        
        \PY{n}{test\PYZus{}data} \PY{o}{=} \PY{n}{norm\PYZus{}test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{troll\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{twitter\PYZus{}df} \PY{o}{=} \PY{n}{twitter\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{norm\PYZus{}to\PYZus{}remove}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
        \PY{n}{troll\PYZus{}df} \PY{o}{=} \PY{n}{troll\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{troll\PYZus{}to\PYZus{}remove}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \hypertarget{generate-word-frequencies-for-both-datasets}{%
\subsection{Generate Word Frequencies For Both
Datasets}\label{generate-word-frequencies-for-both-datasets}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Generate word counts}
        \PY{n}{word\PYZus{}count} \PY{o}{=} \PY{n}{generate\PYZus{}word\PYZus{}count}\PY{p}{(}\PY{n}{troll\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Generate the word frequencies}
        \PY{n}{troll\PYZus{}wf} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{o}{.}\PY{n}{from\PYZus{}dict}\PY{p}{(}\PY{n}{word\PYZus{}count}\PY{p}{,} \PY{n}{orient}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{n}{total\PYZus{}count} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{troll\PYZus{}wf}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n}{troll\PYZus{}wf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word\PYZus{}freq}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{troll\PYZus{}wf}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{generate\PYZus{}word\PYZus{}freq}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{total\PYZus{}count}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{troll\PYZus{}wf}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:}                          0     word\_freq
        rt                  147998  5.273072e-02
        the                  69224  2.466406e-02
        to                   55419  1.974543e-02
        a                    38953  1.387870e-02
        of                   33860  1.206410e-02
        in                   32340  1.152253e-02
        is                   29449  1.049249e-02
        trump                27347  9.743558e-03
        and                  26415  9.411492e-03
        for                  24829  8.846411e-03
        you                  22614  8.057221e-03
        i                    20263  7.219575e-03
        on                   18274  6.510907e-03
        this                 13787  4.912218e-03
        that                 13230  4.713763e-03
        it                   13140  4.681696e-03
        with                 11964  4.262695e-03
        are                  11520  4.104501e-03
        be                   11294  4.023979e-03
        clinton              10887  3.878967e-03
        hillary              10454  3.724692e-03
        not                  10136  3.611391e-03
        we                    9104  3.243696e-03
        amp                   9100  3.242271e-03
        at                    9061  3.228375e-03
        my                    8922  3.178850e-03
        your                  8646  3.080513e-03
        have                  8347  2.973982e-03
        will                  8316  2.962937e-03
        obama                 8039  2.864243e-03
        {\ldots}                    {\ldots}           {\ldots}
        pascalguyon              1  3.562935e-07
        httpstcooyqomdnop        1  3.562935e-07
        wallsmart                1  3.562935e-07
        smileynh                 1  3.562935e-07
        markawebster             1  3.562935e-07
        httpstcobsbumoux         1  3.562935e-07
        biffy                    1  3.562935e-07
        httpstcozbpbzado         1  3.562935e-07
        httpstcolvzsy            1  3.562935e-07
        sofarthisyear            1  3.562935e-07
        httpstcocitp             1  3.562935e-07
        httpstcoufldwqwbel       1  3.562935e-07
        taxestruth               1  3.562935e-07
        httpstcotscddzzk         1  3.562935e-07
        realdonaltrump           1  3.562935e-07
        httpstcocewtimyh         1  3.562935e-07
        httpstcosdxcxoukt        1  3.562935e-07
        ensur                    1  3.562935e-07
        httpstcootzbcayx         1  3.562935e-07
        sirano                   1  3.562935e-07
        pmchamberlain            1  3.562935e-07
        httpstcojhyatgv          1  3.562935e-07
        stanagainstevil          1  3.562935e-07
        mcmoynihan               1  3.562935e-07
        musetto                  1  3.562935e-07
        httptconfoytuxzk         1  3.562935e-07
        probama                  1  3.562935e-07
        httpstcofkytlbsz         1  3.562935e-07
        httpstconxaeo            1  3.562935e-07
        qual                     1  3.562935e-07
        
        [234042 rows x 2 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} Generate word counts}
        \PY{n}{twitter\PYZus{}word\PYZus{}count} \PY{o}{=} \PY{n}{generate\PYZus{}word\PYZus{}count}\PY{p}{(}\PY{n}{twitter\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Generate word frequencies}
        \PY{n}{twit\PYZus{}wf} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{o}{.}\PY{n}{from\PYZus{}dict}\PY{p}{(}\PY{n}{twitter\PYZus{}word\PYZus{}count}\PY{p}{,} \PY{n}{orient}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{n}{total\PYZus{}count} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{twit\PYZus{}wf}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n}{twit\PYZus{}wf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word\PYZus{}freq}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{twit\PYZus{}wf}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{generate\PYZus{}word\PYZus{}freq}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{total\PYZus{}count}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{twit\PYZus{}wf}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:}                         0  word\_freq
        rt                  12467   0.043787
        the                  7575   0.026605
        to                   5717   0.020079
        i                    5124   0.017997
        a                    4918   0.017273
        you                  3919   0.013764
        and                  3805   0.013364
        is                   3302   0.011597
        of                   3135   0.011011
        in                   2868   0.010073
        this                 2707   0.009508
        for                  2477   0.008700
        my                   2317   0.008138
        me                   2158   0.007579
        that                 2006   0.007046
        it                   1959   0.006880
        on                   1896   0.006659
        be                   1430   0.005022
        with                 1386   0.004868
        im                   1337   0.004696
        so                   1285   0.004513
        your                 1210   0.004250
        not                  1170   0.004109
        if                   1161   0.004078
        like                 1160   0.004074
        are                  1157   0.004064
        have                 1109   0.003895
        just                 1072   0.003765
        at                   1051   0.003691
        all                  1018   0.003575
        {\ldots}                   {\ldots}        {\ldots}
        georgepapa              1   0.000004
        soothing                1   0.000004
        httpstcoahxcimmwc       1   0.000004
        runway                  1   0.000004
        lalisagbt               1   0.000004
        httpstcoelrbeece        1   0.000004
        memearchive             1   0.000004
        criten                  1   0.000004
        httpstcomshrqrmxqi      1   0.000004
        httpstcocbqhluxopf      1   0.000004
        sarahworseryer          1   0.000004
        httpstcoavucjapqrm      1   0.000004
        clarks                  1   0.000004
        leith                   1   0.000004
        lucy                    1   0.000004
        badpostlucy             1   0.000004
        tera                    1   0.000004
        diyanitara              1   0.000004
        rnvirgo                 1   0.000004
        parthianpanian          1   0.000004
        chayani                 1   0.000004
        saumyasomya             1   0.000004
        pournimag               1   0.000004
        lolatomybunny           1   0.000004
        vaishu                  1   0.000004
        lbsssssss               1   0.000004
        httpstcoziuogc          1   0.000004
        httpstconjzsgyof        1   0.000004
        jenbossbish             1   0.000004
        httpstcomnmlxphv        1   0.000004
        
        [43430 rows x 2 columns]
\end{Verbatim}
            
    \hypertarget{classification}{%
\subsection{Classification}\label{classification}}

Consider the words in a given tweet as \(T\). Let us also denote the
troll/bot writer as \(B\) and the normal twitter user as \(N\).

We want to find

\[\frac{P(N|T)}{P(B|T)}\]\\
\[ = \frac{P(T|N)P(N)}{P(T|B)P(B)}\]

Googling around for the percentage of tweets posted by bots indicates
some alarming statistics (see:
https://www.pewresearch.org/fact-tank/2018/04/09/5-things-to-know-about-bots-on-twitter/),
but none of these statistics give a valid prior for the probability of a
tweet being posted by a bot. To represent this ambiguity, we will say
that

\[P(N) = P(B) = 0.5\]

We observe that this cancels out in our equation above, so we are left
with

\[\frac{P(T|N)}{P(T|B)}\]

As we did in class, we can rewrite these using multinomials, and the
multinomial terms in the numerator and denominator cancel, yielding

\[\frac{\prod_i p_{i}^{c_i}}{\prod_i q_{i}^{c_i}}\]

We can use logarithms to make this computationally stable, and write

\[\log(\frac{P(T|N)}{P(T|B)}) = \log(\frac{\prod_i p_{i}^{c_i}}{\prod_i q_{i}^{c_i}}) = \sum_i c_i\log(p_i) - \sum_i c_i \log(q_i)\]

(To reiterate, this process is identical to the process done in lecture
with the Federalist Papers, so some steps in the math were ommited)

Now to convert this to code!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k}{def} \PY{n+nf}{calculate\PYZus{}LL}\PY{p}{(}\PY{n}{word\PYZus{}list}\PY{p}{,} \PY{n}{is\PYZus{}bot}\PY{p}{)}\PY{p}{:}
             \PY{n}{unique\PYZus{}word\PYZus{}error} \PY{o}{=} \PY{l+m+mf}{0.000004}
             \PY{n+nb}{sum} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{freq\PYZus{}list} \PY{o}{=} \PY{k+kc}{None}
             \PY{k}{if} \PY{n}{is\PYZus{}bot}\PY{p}{:}
                 \PY{n}{freq\PYZus{}list} \PY{o}{=} \PY{n}{troll\PYZus{}wf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word\PYZus{}freq}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{k}{else}\PY{p}{:}
                 \PY{n}{freq\PYZus{}list} \PY{o}{=} \PY{n}{twit\PYZus{}wf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word\PYZus{}freq}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{word\PYZus{}list}\PY{p}{:}
                 \PY{k}{if} \PY{p}{(}\PY{n}{word} \PY{o+ow}{in} \PY{n}{freq\PYZus{}list}\PY{p}{)}\PY{p}{:}
                     \PY{n+nb}{sum} \PY{o}{+}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{freq\PYZus{}list}\PY{p}{[}\PY{n}{word}\PY{p}{]}\PY{p}{)}
                 \PY{k}{else}\PY{p}{:}
                     \PY{n+nb}{sum} \PY{o}{+}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{unique\PYZus{}word\PYZus{}error}\PY{p}{)}
             \PY{k}{return} \PY{n+nb}{sum}
         
         \PY{k}{def} \PY{n+nf}{is\PYZus{}troll}\PY{p}{(}\PY{n}{likelihood\PYZus{}norm}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{p}{(}\PY{n}{likelihood\PYZus{}norm} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{k}{return} \PY{k+kc}{False}
             \PY{k}{else}\PY{p}{:}
                 \PY{k}{return} \PY{k+kc}{True}
             
         \PY{k}{def} \PY{n+nf}{print\PYZus{}results}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
         	\PY{n}{class0\PYZus{}count} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{k+kc}{False}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         	\PY{n}{class1\PYZus{}count} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{k+kc}{True}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         	\PY{n}{class0\PYZus{}correct} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred\PYZus{}correct}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{k+kc}{True}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         	\PY{n}{class1\PYZus{}correct} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred\PYZus{}correct}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{k+kc}{True}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         	\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Normal Tweets: tested }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, correctly classified }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{class0\PYZus{}count}\PY{p}{,} \PY{n}{class0\PYZus{}correct}\PY{p}{)}\PY{p}{)}
         	\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Troll Tweets: tested }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, correctly classified }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{class1\PYZus{}count}\PY{p}{,} \PY{n}{class1\PYZus{}correct}\PY{p}{)}\PY{p}{)}
         	\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Overall: tested }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, correctly classified }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{class0\PYZus{}count} \PY{o}{+} \PY{n}{class1\PYZus{}count}\PY{p}{,} \PY{n}{class0\PYZus{}correct} \PY{o}{+} \PY{n}{class1\PYZus{}correct}\PY{p}{)}\PY{p}{)}
         	\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{(}\PY{n}{class0\PYZus{}correct} \PY{o}{+} \PY{n}{class1\PYZus{}correct}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{class0\PYZus{}count} \PY{o}{+} \PY{n}{class1\PYZus{}count}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{norm\PYZus{}LL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{calculate\PYZus{}LL}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{k+kc}{False}\PY{p}{)}\PY{p}{)}
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}LL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{calculate\PYZus{}LL}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{norm\PYZus{}LL \PYZhy{} bot\PYZus{}LL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{norm\PYZus{}LL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bot\PYZus{}LL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{e\PYZca{}(prob)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{norm\PYZus{}LL \PYZhy{} bot\PYZus{}LL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred\PYZus{}is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{e\PYZca{}(prob)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{is\PYZus{}troll}\PY{p}{)}
         \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred\PYZus{}correct}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}data}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred\PYZus{}is\PYZus{}troll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{print\PYZus{}results}\PY{p}{(}\PY{n}{test\PYZus{}data}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Normal Tweets: tested 2500, correctly classified 2247.
Troll Tweets: tested 2500, correctly classified 1889.
Overall: tested 5000, correctly classified 4136.
Accuracy = 0.8272.

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
